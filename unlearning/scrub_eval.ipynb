{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reload \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from importlib import reload \n",
    "\n",
    "CWD =Path.cwd()\n",
    "BASE_DIR = CWD.parent.parent\n",
    "BASE_DIR\n",
    "\n",
    "def read_yaml(f):\n",
    "    with open(f, \"r\") as stream:\n",
    "        try:\n",
    "            return yaml.safe_load(stream)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "\n",
    "from unlearning.auditors import eval_suite\n",
    "reload(eval_suite)\n",
    "\n",
    "forget_set_id = 5\n",
    "retain_data_amount= 2.\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "config_dict = {'results_dir': './results/',\n",
    " 'dataset': 'CIFAR10',\n",
    " 'forget_set_id': 5,\n",
    " 'unlearning_algo': 'scrub',\n",
    " 'run_direct_eval': True,\n",
    " 'use_submitit_for_direct_eval': False,\n",
    " 'unlearning_algo_kwargs': {'dataset': 'CIFAR10',\n",
    "  'forget_set_id': 5,\n",
    "  'oracles_path': '/n/home04/rrinberg/data_dir__holylabs/unlearning/precomputed_models/oracles/CIFAR10/forget_set_5',\n",
    "  'retain_data_amount': 2.0,\n",
    "  'mix_data': True,\n",
    "  'num_epochs': 10},\n",
    " 'reorder_logit_classes': True,\n",
    " 'N_models_for_direct': 20}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as ch\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from utils import (\n",
    "    model_factory,\n",
    "    loader_factory,\n",
    "    load_forget_set_indices,\n",
    "    get_full_model_paths,\n",
    "    get_oracle_paths,\n",
    ")\n",
    "\n",
    "\n",
    "def load_model(path, model_factory, ds_name):\n",
    "    model = model_factory(ds_name)\n",
    "    loaded_model = ch.load(path)\n",
    "    first_key = list(loaded_model.keys())[0]\n",
    "    if \"model\" in first_key:\n",
    "        model.load_state_dict(loaded_model)\n",
    "\n",
    "    else:\n",
    "        # add \".model\" to each key in k,vs\n",
    "        loaded_model = {f\"model.{k}\": v for k, v in loaded_model.items()}\n",
    "        model.load_state_dict(loaded_model)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m unlearn_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscrub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mconfig_dict\u001b[49m\n\u001b[1;32m      7\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config_dict' is not defined"
     ]
    }
   ],
   "source": [
    "forget_set_id =5\n",
    "unlearn_name = \"scrub\"\n",
    "from scrub import scrub_wrapper\n",
    "#####\n",
    "\n",
    "config = config_dict\n",
    "results = {}\n",
    "results[\"params\"] = {}\n",
    "ds_name = \"CIFAR10\"\n",
    "# for now, let's tie the model to the dataset, so we have fewer moving pieces\n",
    "model = model_factory(ds_name)  # on cuda, in eval mode\n",
    "\n",
    "\n",
    "forget_set_indices = load_forget_set_indices(ds_name, forget_set_id)\n",
    "\n",
    "results[\"params\"][\"forget_set_indices\"] = forget_set_indices\n",
    "unlearn_fn = scrub_wrapper\n",
    "\n",
    "unlearning_kwargs = config[\"unlearning_algo_kwargs\"]\n",
    "if unlearning_kwargs is None:\n",
    "    unlearning_kwargs = {}\n",
    "\n",
    "with redirect_stdout(open(\"/dev/null\", \"w\")):\n",
    "    # no shuffling, no augmentation\n",
    "    train_loader = loader_factory(ds_name, indexed=True)\n",
    "    val_loader = loader_factory(ds_name, split=\"val\", indexed=True)\n",
    "    forget_loader = loader_factory(\n",
    "        ds_name,\n",
    "        indices=forget_set_indices,\n",
    "        batch_size=50,\n",
    "        indexed=True,\n",
    "    )\n",
    "    eval_set_inds = np.arange(\n",
    "        len(train_loader.dataset) + len(val_loader.dataset))\n",
    "    eval_loader = loader_factory(ds_name,\n",
    "                                    split=\"train_and_val\",\n",
    "                                    indices=eval_set_inds,\n",
    "                                    indexed=True)\n",
    "####### END OF SETUP ########\n",
    "\n",
    "####### LOAD PRETRAINED MODELS ########\n",
    "# f stands for \"full model\",\n",
    "#   i.e. the model that was trained on the enitre dataset (retain + forget)\n",
    "# o stands for \"oracle model\"\n",
    "#   i.e. the model that was trained on the oracle dataset (retain only)\n",
    "\n",
    "# inserted by Roy for some speed reason\n",
    "splits = [\"train\", \"val\"]\n",
    "\n",
    "f_ckpt_paths, f_logit_paths, f_margins_paths = get_full_model_paths(\n",
    "    ds_name, splits=splits)\n",
    "(\n",
    "    o_ckpt_0_path,  # we only need a single oracle checkpoint\n",
    "    o_logit_paths,\n",
    "    o_margins_paths,\n",
    ") = get_oracle_paths(ds_name, config[\"forget_set_id\"], splits=splits)\n",
    "\n",
    "# original model\n",
    "model = load_model(f_ckpt_paths[0], model_factory, ds_name)\n",
    "\n",
    "# oracle model\n",
    "oracle_model = load_model(o_ckpt_0_path, model_factory, ds_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation\n",
      "Files already downloaded and verified\n",
      "No augmentation\n",
      "Files already downloaded and verified\n",
      "total epochs : 10\n",
      "Epoch 1 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 2 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 3 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 4 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 5 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 6 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 7 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 8 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 9 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 10 ...\n",
      " * Acc@1 100.000 \n",
      "update params\n"
     ]
    }
   ],
   "source": [
    "# unlearning\n",
    "unlearned_model = unlearn_fn(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    forget_dataloader=None,\n",
    "    forget_indices=forget_set_indices,\n",
    "    **unlearning_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original to Unlearned: 0.034105591600308044\n",
      "Original to Oracle: 35.88708313018628\n",
      "Unlearned to Oracle: 35.8764929755155\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def l2_difference(model1, model2):\n",
    "    l2_diff = 0.0\n",
    "    # Ensure both models are in the same state (e.g., both in eval mode)\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (param1, param2) in zip(model1.parameters(), model2.parameters()):\n",
    "            # Check if both parameters are on the same device and are of the same shape\n",
    "            if param1.device != param2.device or param1.shape != param2.shape:\n",
    "                raise ValueError(\"Models have parameters on different devices or with different shapes\")\n",
    "            \n",
    "            # Compute the squared L2 norm of the difference between the parameters\n",
    "            param_diff = param1 - param2\n",
    "            l2_diff += torch.norm(param_diff, p=2).item()**2\n",
    "\n",
    "    # Return the square root of the sum of squared differences\n",
    "    return l2_diff**0.5\n",
    "\n",
    "original_to_d = l2_difference(model, unlearned_model)\n",
    "original_to_oracle = l2_difference(model, oracle_model)\n",
    "unlearned_to_oracle = l2_difference(oracle_model, unlearned_model)\n",
    "print(f\"Original to Unlearned: {original_to_d}\")\n",
    "print(f\"Original to Oracle: {original_to_oracle}\")\n",
    "print(f\"Unlearned to Oracle: {unlearned_to_oracle}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
