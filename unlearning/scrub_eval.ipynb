{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/usr/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "from unlearning.auditors import eval_suite\n",
    "\n",
    "reload(eval_suite)\n",
    "\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "\n",
    "from contextlib import redirect_stdout\n",
    "\n",
    "from utils import (\n",
    "    model_factory,\n",
    "    loader_factory,\n",
    ")\n",
    "\n",
    "from scrub import scrub_wrapper\n",
    "CWD = Path.cwd()\n",
    "BASE_DIR = CWD.parent.parent\n",
    "\n",
    "retain_data_amount = 2.\n",
    "num_epochs = 5\n",
    "\n",
    "config_dict = {\n",
    "    'results_dir': './results/',\n",
    "    'dataset': 'CIFAR10',\n",
    "    'forget_set_id': 5,\n",
    "    'unlearning_algo': 'scrub',\n",
    "    'run_direct_eval': True,\n",
    "    'use_submitit_for_direct_eval': False,\n",
    "    'unlearning_algo_kwargs': {\n",
    "        'dataset': 'CIFAR10',\n",
    "        'forget_set_id': 5,\n",
    "        'oracles_path':\n",
    "        '/n/home04/rrinberg/data_dir__holylabs/unlearning/precomputed_models/oracles/CIFAR10/forget_set_5',\n",
    "        'retain_data_amount': 2.0,\n",
    "        'mix_data': True,\n",
    "        'num_epochs': 10\n",
    "    },\n",
    "    'reorder_logit_classes': True,\n",
    "    'N_models_for_direct': 20\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(path, model_factory, ds_name):\n",
    "    model = model_factory(ds_name)\n",
    "    loaded_model = ch.load(path)\n",
    "    first_key = list(loaded_model.keys())[0]\n",
    "    if \"model\" in first_key:\n",
    "        model.load_state_dict(loaded_model)\n",
    "\n",
    "    else:\n",
    "        # add \".model\" to each key in k,vs\n",
    "        loaded_model = {f\"model.{k}\": v for k, v in loaded_model.items()}\n",
    "        model.load_state_dict(loaded_model)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "unlearn_name = \"scrub\"\n",
    "\n",
    "#####\n",
    "config = config_dict\n",
    "results = {}\n",
    "results[\"params\"] = {}\n",
    "ds_name = \"CIFAR10\"\n",
    "# for now, let's tie the model to the dataset, so we have fewer moving pieces\n",
    "model = model_factory(ds_name)  # on cuda, in eval mode\n",
    "\n",
    "forget_set_indices = [305, 1346, 1538, 2335, 3799, 4260, 4956, 5894, 6873, 7364, 7398, 7531, 7726, 8050, 9196, 9235, 9377, 9665, 9999, 10221, 10482, 12132, 12300, 14355, 14667, 15103, 15602, 16905, 17306, 17400, 18014, 18278, 18512, 18912, 19222, 19231, 19285, 19606, 21191, 21480, 21502, 22321, 22487, 22749, 22876, 22908, 23369, 23385, 23898, 23914, 24637, 25886, 26388, 28340, 28510, 28612, 28726, 28973, 29242, 29271, 29712, 29795, 30156, 30523, 31017, 31129, 31781, 31875, 33079, 33735, 34516, 35932, 36319, 36454, 36871, 37316, 37471, 37589, 39645, 39880, 40004, 40663, 40800, 42396, 42731, 43209, 44581, 45278, 45477, 46146, 46264, 46715, 47482, 47716, 47729, 48169, 48980, 49014, 49316, 49753]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m unlearn_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscrub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#####\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mconfig_dict\u001b[49m\n\u001b[1;32m      7\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      8\u001b[0m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config_dict' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "results[\"params\"][\"forget_set_indices\"] = forget_set_indices\n",
    "unlearn_fn = scrub_wrapper\n",
    "\n",
    "unlearning_kwargs = config[\"unlearning_algo_kwargs\"]\n",
    "\n",
    "if unlearning_kwargs is None:\n",
    "    unlearning_kwargs = {}\n",
    "\n",
    "with redirect_stdout(open(\"/dev/null\", \"w\")):\n",
    "    # no shuffling, no augmentation\n",
    "    train_loader = loader_factory(ds_name, indexed=True)\n",
    "    val_loader = loader_factory(ds_name, split=\"val\", indexed=True)\n",
    "    forget_loader = loader_factory(\n",
    "        ds_name,\n",
    "        indices=forget_set_indices,\n",
    "        batch_size=50,\n",
    "        indexed=True,\n",
    "    )\n",
    "    eval_set_inds = np.arange(\n",
    "        len(train_loader.dataset) + len(val_loader.dataset))\n",
    "    eval_loader = loader_factory(ds_name,\n",
    "                                 split=\"train_and_val\",\n",
    "                                 indices=eval_set_inds,\n",
    "                                 indexed=True)\n",
    "####### END OF SETUP ########\n",
    "\n",
    "####### LOAD PRETRAINED MODELS ########\n",
    "# original model\n",
    "original_model_path = CWD / \"full_model.pt\"\n",
    "oracle_model_path = CWD / \"retrained_oracle.pt\"\n",
    "model = load_model(original_model_path, model_factory, ds_name)\n",
    "\n",
    "# oracle model\n",
    "oracle_model = load_model(oracle_model_path, model_factory, ds_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No augmentation\n",
      "Files already downloaded and verified\n",
      "No augmentation\n",
      "Files already downloaded and verified\n",
      "total epochs : 10\n",
      "Epoch 1 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 2 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 3 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 4 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 5 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 6 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 7 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 8 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 9 ...\n",
      " * Acc@1 100.000 \n",
      "Epoch 10 ...\n",
      " * Acc@1 100.000 \n",
      "update params\n"
     ]
    }
   ],
   "source": [
    "# unlearning\n",
    "unlearned_model = unlearn_fn(\n",
    "    model=model,\n",
    "    train_dataloader=train_loader,\n",
    "    forget_dataloader=None,\n",
    "    forget_indices=forget_set_indices,\n",
    "    **unlearning_kwargs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original to Unlearned: 0.034105591600308044\n",
      "Original to Oracle: 35.88708313018628\n",
      "Unlearned to Oracle: 35.8764929755155\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def l2_difference(model1, model2):\n",
    "    l2_diff = 0.0\n",
    "    # Ensure both models are in the same state (e.g., both in eval mode)\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (param1, param2) in zip(model1.parameters(), model2.parameters()):\n",
    "            # Check if both parameters are on the same device and are of the same shape\n",
    "            if param1.device != param2.device or param1.shape != param2.shape:\n",
    "                raise ValueError(\"Models have parameters on different devices or with different shapes\")\n",
    "            \n",
    "            # Compute the squared L2 norm of the difference between the parameters\n",
    "            param_diff = param1 - param2\n",
    "            l2_diff += torch.norm(param_diff, p=2).item()**2\n",
    "\n",
    "    # Return the square root of the sum of squared differences\n",
    "    return l2_diff**0.5\n",
    "\n",
    "original_to_d = l2_difference(model, unlearned_model)\n",
    "original_to_oracle = l2_difference(model, oracle_model)\n",
    "unlearned_to_oracle = l2_difference(oracle_model, unlearned_model)\n",
    "print(f\"Original to Unlearned: {original_to_d}\")\n",
    "print(f\"Original to Oracle: {original_to_oracle}\")\n",
    "print(f\"Unlearned to Oracle: {unlearned_to_oracle}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
